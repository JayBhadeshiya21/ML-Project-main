{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6c6c38-7c09-4cf5-be58-056834cc279d",
   "metadata": {},
   "source": [
    "Step 1) Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85fb6b74-af3d-4f3d-80f8-cfc53933a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b7143-7266-4bed-bd0e-058bf7f93038",
   "metadata": {},
   "source": [
    "Step 2) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26789599-daed-4edc-ae46-7e0a5ab825f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('cardio.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f1f32-ca19-490a-8deb-d2f0787296ab",
   "metadata": {},
   "source": [
    "Step 3) Separate Input & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "093ee4cb-22f6-4d80-bf39-271a55d1df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('cardio', axis=1)\n",
    "y = df['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5fbffa-d2c9-42ab-b9be-21ab1f3f3a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000e+00, 5.0000e+01, 1.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       [1.0000e+00, 5.5000e+01, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       [2.0000e+00, 5.2000e+01, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       ...,\n",
       "       [6.9995e+04, 5.3000e+01, 1.0000e+00, ..., 1.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00],\n",
       "       [6.9998e+04, 6.1000e+01, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00],\n",
       "       [6.9999e+04, 5.6000e+01, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00,\n",
       "        1.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a89ee-abb7-4483-ba9e-73b53fae89a9",
   "metadata": {},
   "source": [
    "X.values= This is done for faster Calculaitons   and it is called NumpyArray\n",
    "np.random.seed(42) This is done to ensure same data split is done everytime\n",
    "np.random.permutation this len(X_np) it shows th no of rows and we have to shuffle it and make it more random\n",
    "split_index this is doen for trianing data to be 80 percent of data first are taken for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcce04b-a0af-4a5b-acd6-7ccc6bc1e382",
   "metadata": {},
   "source": [
    "Step 4) Manual Trainâ€“Test Split (NO LIBRARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff0cd18-de5e-4a64-962a-92cb55f64b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.values\n",
    "Y_np = y.values\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len (X_np))\n",
    "split_index = int(0.8 * len(X_np))\n",
    "train_idx =  indices[:split_index]\n",
    "test_idx = indices[split_index:]\n",
    "X_train = X_np[train_idx]\n",
    "X_test = X_np[test_idx]\n",
    "y_train = Y_np[train_idx]\n",
    "y_test = Y_np[test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b22a0-44a4-4693-93d1-15e8b0354a2e",
   "metadata": {},
   "source": [
    "Step 5) Initialize Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496fc7c-3847-46ad-b4e1-1cb1e0bacd0c",
   "metadata": {},
   "source": [
    "Epochs =1000 this shows that our model will learn 1000 times it will iterate and learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b406bd2-66db-4594-bb3e-639ad2dcdb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(X_train.shape[1])\n",
    "bias =0\n",
    "learning_rate = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552aeb7-25ef-4821-baa2-9f0ff7a66bf7",
   "metadata": {},
   "source": [
    "Step 6) Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67131a26-91a0-4bd7-be52-16a70a88469b",
   "metadata": {},
   "source": [
    "any number it will convert it to Probability between zero and one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b624f005-14cb-4546-bc68-3482e14f7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+ np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fac77a-d9cf-4c4f-b554-8da64c1c0969",
   "metadata": {},
   "source": [
    "Step 7 Gradient Descent For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abfe2fcd-4158-4132-a86e-77aab32cbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_11140\\3813440747.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1+ np.exp(-z))\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Training using Gradient Descent\n",
    "for i in range(epochs):\n",
    "    # 1. Calculate linear combination\n",
    "    z = np.dot(X_train, weights) + bias\n",
    "    \n",
    "    # 2. Apply stable sigmoid\n",
    "    y_predicted = sigmoid(z)\n",
    "    \n",
    "    # 3. Calculate gradients\n",
    "    dw = (1 / X_train.shape[0]) * np.dot(X_train.T, (y_predicted - y_train))\n",
    "    db = (1 / X_train.shape[0]) * np.sum(y_predicted - y_train)\n",
    "    \n",
    "    # 4. Update weights and bias\n",
    "    weights = weights - learning_rate * dw\n",
    "    bias = bias - learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85dfdeac-7ab0-4e6a-95dc-a27fe95e3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Stable sigmoid to avoid overflow\n",
    "    return np.where(z >= 0, 1 / (1 + np.exp(-z)), np.exp(z) / (1 + np.exp(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45073172-99cf-44c4-ae7b-50ba91c9578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max scaling (scale values between 0 and 1)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_test = (X_test - X_test.min(axis=0)) / (X_test.max(axis=0) - X_test.min(axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ba448-3260-43ac-a5e0-db1c07a12ecb",
   "metadata": {},
   "source": [
    "Step 8: Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b8e174-1b59-49a8-86cb-3399318546ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate linear combination for test data\n",
    "z_test = np.dot(X_test, weights) + bias\n",
    "\n",
    "# 2. Apply sigmoid to get probabilities\n",
    "y_test_predicted = sigmoid(z_test)\n",
    "\n",
    "# 3. Convert probabilities to class labels (0 or 1)\n",
    "y_test_predicted_class = [1 if i > 0.5 else 0 for i in y_test_predicted]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e6b40-eb46-4328-8fc6-c9d7afa5b727",
   "metadata": {},
   "source": [
    "Step 9: Check Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dda8991-3814-472d-901c-4238d00d229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5012399008079353\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = np.mean(y_test_predicted_class == y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55505b49-e38c-4937-8b6a-c21113543741",
   "metadata": {},
   "source": [
    "Step 10 : Confusion Matrix (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fecb5f83-91b8-4d8b-be37-f117ef497d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tp, fp],\n",
    "                     [fn, tn]])\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_predicted_class)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c593689-007b-4030-a8ed-148a6a95dacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         1.         1.         1.\n",
      " 0.99999977 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_predicted[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921382f2-850f-4a73-b23e-537f54d44f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_class = np.array([1 if i > 0.5 else 0 for i in y_test_predicted]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "915bec9e-5638-4fe5-972c-2d485ff54ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_predicted_class[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cfb5b12-5b03-4eec-870a-81c663b2ecaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test_np \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_test_np[:\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "y_test_np = y_test.to_numpy().astype(int)\n",
    "print(y_test_np[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c21b0ea-ded0-4ef9-a270-d0aaec4a4b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# If y_test is already a NumPy array\n",
    "y_test_np = y_test.astype(int)\n",
    "print(y_test_np[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "220c645c-4a44-4987-bfb0-2ea0fb1115f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[6266 6235]\n",
      " [   0    0]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return np.array([[tp, fp],\n",
    "                     [fn, tn]])\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_test_predicted_class)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597429ec-a68f-4f65-bbec-5cc5bf4236d9",
   "metadata": {},
   "source": [
    "Accuracy Increase karva mate pela hati 50 Percentage Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1318428-b686-4a23-b08a-af7841b7af32",
   "metadata": {},
   "source": [
    "STEP 1: FEATURE SCALING (MOST IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8c69516-abfa-427e-b890-e5d131b6042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (Standardization)\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb05cc2-59a8-42fc-a84e-1be1433e6335",
   "metadata": {},
   "source": [
    "STEP 2: STABLE SIGMOID (NO OVERFLOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb8f7193-25b2-4570-ae9b-7639a85feb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return np.where(z >= 0,\n",
    "                    1 / (1 + np.exp(-z)),\n",
    "                    np.exp(z) / (1 + np.exp(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac52a2ce-8ee5-46e6-a3b6-3d5370c11379",
   "metadata": {},
   "source": [
    "STEP 3: RE-INITIALIZE PARAMETERS (RESET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6538f85-e565-4735-befe-a0357c97c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros(X_train.shape[1])\n",
    "bias = 0\n",
    "learning_rate = 0.01\n",
    "epochs = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5aeeda-bc5d-4943-a55a-9677c37cdd6f",
   "metadata": {},
   "source": [
    "STEP 4: TRAIN AGAIN (GRADIENT DESCENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00ccfb29-db42-43b9-90ef-320aa9fa2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    z = np.dot(X_train, weights) + bias\n",
    "    y_predicted = sigmoid(z)\n",
    "\n",
    "    dw = (1 / X_train.shape[0]) * np.dot(X_train.T, (y_predicted - y_train))\n",
    "    db = (1 / X_train.shape[0]) * np.sum(y_predicted - y_train)\n",
    "\n",
    "    weights -= learning_rate * dw\n",
    "    bias -= learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00951a-377c-4e2e-80e3-4ac84a8e079a",
   "metadata": {},
   "source": [
    "STEP 5: PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa665fc-111d-4ac2-bbf1-5ae4fbb00806",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = np.dot(X_test, weights) + bias\n",
    "y_test_pred = sigmoid(z_test)\n",
    "y_test_pred_class = (y_test_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad07a31-4b88-4bc3-a511-c64699a666f1",
   "metadata": {},
   "source": [
    "STEP 6: ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7596be4a-b25a-4dbe-b785-dbb785642613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7192224622030238\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_test_pred_class == y_test)\n",
    "print(\"Accuracy:\", accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee89968a-5de0-428b-8320-251958736f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Converting age to years...\n",
      "Training Model...\n",
      "Saving model file...\n",
      "SUCCESS: model/cardio_model.pkl created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# 1. Create the model directory if it doesn't exist\n",
    "if not os.path.exists('model'):\n",
    "    os.makedirs('model')\n",
    "\n",
    "# 2. Load your dataset\n",
    "print(\"Loading dataset...\")\n",
    "# Make sure this matches your file name exactly\n",
    "df = pd.read_csv('ds_cvd_w1.csv') \n",
    "\n",
    "# 3. Clean and Train\n",
    "# Drop ID if present\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "# Convert age from days to years if needed\n",
    "if df['age'].mean() > 150:\n",
    "    print(\"Converting age to years...\")\n",
    "    df['age'] = (df['age'] / 365.25).round().astype(int)\n",
    "\n",
    "X = df.drop(columns=['cardio'])\n",
    "y = df['cardio']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Model...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Save the model for the App\n",
    "print(\"Saving model file...\")\n",
    "joblib.dump(model, 'model/cardio_model.pkl')\n",
    "print(\"SUCCESS: model/cardio_model.pkl created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0088d-15a6-4c7b-901c-724266a7fdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
